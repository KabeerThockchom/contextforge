Metadata-Version: 2.4
Name: contextforge
Version: 0.1.0
Summary: A Python package for building LLM applications with sophisticated context engineering
Home-page: https://github.com/yourusername/contextforge
Author: Your Name
Author-email: Your Name <your.email@example.com>
License: MIT
Project-URL: Homepage, https://github.com/yourusername/contextforge
Project-URL: Bug Tracker, https://github.com/yourusername/contextforge/issues
Project-URL: Documentation, https://github.com/yourusername/contextforge#readme
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: aiohttp>=3.8.0
Requires-Dist: numpy>=1.21.0
Requires-Dist: python-dateutil>=2.8.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.20.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: pytest-mock>=3.10.0; extra == "dev"
Requires-Dist: black>=22.0.0; extra == "dev"
Requires-Dist: flake8>=5.0.0; extra == "dev"
Requires-Dist: mypy>=0.990; extra == "dev"
Provides-Extra: embeddings
Requires-Dist: sentence-transformers>=2.2.0; extra == "embeddings"
Requires-Dist: openai>=1.0.0; extra == "embeddings"
Provides-Extra: jupyter
Requires-Dist: jupyter>=1.0.0; extra == "jupyter"
Requires-Dist: ipykernel>=6.0.0; extra == "jupyter"
Requires-Dist: notebook>=6.0.0; extra == "jupyter"
Dynamic: author
Dynamic: home-page
Dynamic: license-file
Dynamic: requires-python

# README.md
# ContextForge

A powerful Python package for building LLM applications with sophisticated context engineering. ContextForge simplifies the process of managing conversations, memory, tools, and retrieval-augmented generation (RAG) across multiple LLM providers.

## Features

- **Multi-Provider Support**: Seamlessly switch between OpenAI, Anthropic, Ollama, and custom providers
- **Streaming Responses**: Real-time token-by-token streaming for better user experience
- **Context Engineering**: Implements the complete context engineering pattern with system prompts, instructions, memory management, and tool integration
- **Conversation Management**: Automatic session handling with conversation history
- **Memory Systems**: Both short-term and long-term memory with SQLite persistence
- **Tool Integration**: Easy function calling with automatic parameter extraction
- **RAG Support**: Built-in retrieval system for augmenting responses with relevant information
- **Async-First**: Fully asynchronous for optimal performance
- **Type-Safe**: Comprehensive type hints throughout

## Installation

```bash
pip install contextforge
```

For additional features:
```bash
pip install contextforge[embeddings]  # For advanced embedding support
pip install contextforge[dev]         # For development tools
```

## Quick Start

```python
import asyncio
from contextforge import ContextEngine

async def main():
    # Initialize the engine
    engine = ContextEngine(
        provider="openai",  # or "anthropic", "ollama"
        default_system_prompt="You are a helpful assistant."
    )
    
    # Simple generation
    response = await engine.generate("What is the capital of France?")
    print(response)
    
    # Follow-up with automatic context
    response = await engine.generate("What is its population?")
    print(response)
    
    # Streaming response
    print("Assistant: ", end="", flush=True)
    async for chunk in await engine.generate("Tell me a story", stream=True):
        print(chunk, end="", flush=True)

asyncio.run(main())
```

## Advanced Usage

### Using Tools

```python
from contextforge import ContextEngine, ToolRegistry

# Create tool registry
tools = ToolRegistry()

@tools.register(description="Search the web")
def web_search(query: str) -> str:
    return f"Results for {query}..."

@tools.register(description="Calculate math")
def calculate(expression: str) -> float:
    return eval(expression)

# Use with engine
engine = ContextEngine(provider="openai", tool_registry=tools)
response = await engine.generate("Search for Python tutorials", use_tools=True)
```

### Retrieval-Augmented Generation

```python
from contextforge import ContextEngine, VectorRetriever

# Setup retriever
retriever = VectorRetriever()
await retriever.add_document({
    "content": "Paris is the capital of France with 2.2 million residents.",
    "source": "Wikipedia"
})

# Use with engine
engine = ContextEngine(provider="anthropic", retriever=retriever)
response = await engine.generate("Tell me about Paris", retrieve=True)
```

### Streaming Responses

```python
# Basic streaming
async for chunk in await engine.generate("Tell me a story", stream=True):
    print(chunk, end="", flush=True)

# Streaming with callbacks (production pattern)
async def on_token(token: str):
    # Send to frontend via WebSocket
    await websocket.send(token)

response_chunks = []
async for chunk in await engine.generate(prompt, stream=True):
    response_chunks.append(chunk)
    await on_token(chunk)

full_response = "".join(response_chunks)
```

### Structured Output

```python
schema = {
    "type": "object",
    "properties": {
        "name": {"type": "string"},
        "age": {"type": "integer"}
    }
}

response = await engine.generate(
    "Generate a person profile",
    output_schema=schema
)
```

## Documentation

Full documentation is available at [docs.contextforge.dev](https://docs.contextforge.dev).

## Contributing

Contributions are welcome! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

## License

MIT License - see [LICENSE](LICENSE) file for details.
